{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from math import log\n",
    "\n",
    "%run tokenization.ipynb\n",
    "\n",
    "class BestMatchModel:\n",
    "    def __init__(self):\n",
    "        # variable initial         \n",
    "        self.total_doc_length = 0\n",
    "        self.avg_doclen = 0\n",
    "        self.docs_length = dict()\n",
    "        self.doc_list = dict()\n",
    "        self.query_list = dict()\n",
    "        self.vocab = dict()\n",
    "        self.queries = dict()\n",
    "        \n",
    "        # score variable\n",
    "        self.score_list = dict()\n",
    "        self.rank = dict()\n",
    "        \n",
    "        # preprocess tool \n",
    "        self.tokenize = Tokenization()\n",
    "    \n",
    "    def set_docs_path(self, doc_list_path, doc_fpath):\n",
    "        self.doc_list_path = doc_list_path\n",
    "        self.doc_fpath = doc_fpath\n",
    "        \n",
    "    def set_queries_path(self, query_list_path, query_fpath):\n",
    "        self.query_list_path = query_list_path\n",
    "        self.query_fpath = query_fpath\n",
    "        \n",
    "    def get_file_list(self, file_list_path):      \n",
    "        \n",
    "        file_list = list()        \n",
    "        with open(file_list_path, 'r', encoding='UTF-8') as f:\n",
    "            for file_id in f.readlines():\n",
    "                file_list.append(file_id.strip('\\n'))\n",
    "        f.close()\n",
    "        \n",
    "        return file_list\n",
    "    \n",
    "    def read_tokenize_data(self, file_path):\n",
    "        \n",
    "        with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "            text = self.tokenize.cut(f.read(), stopword=False, splitnum=False)\n",
    "            # text = f.read()            \n",
    "        f.close()\n",
    "            \n",
    "        return text.split()\n",
    "    \n",
    "    def view_df(self):\n",
    "        df = dict()\n",
    "        for term in self.vocab:\n",
    "            df[term] = self.vocab[term]['df']\n",
    "            \n",
    "        df_list = [ (key, value) for key, value in sorted(df.items(),\n",
    "                      key = lambda item:item[1], reverse=True) ]\n",
    "        \n",
    "        for term, df in df_list:\n",
    "            print(term, df)\n",
    "    \n",
    "    ''' BM Model factor calculate method '''\n",
    "    \n",
    "    # calc_F_prime\n",
    "    def calc_doc_tf(self, tf, doc_len):\n",
    "        return ( (self.k1+1) * (tf+self.delta) / ( self.k1 * ( (1-self.b)+ self.b * doc_len/self.avg_doclen) + tf + self.delta) )\n",
    "\n",
    "    # calc_F\n",
    "    def calc_query_tf(self, tf):     \n",
    "        return ( (self.k3+1)*tf ) / ( self.k3 + tf )\n",
    "    \n",
    "    # calc_Sparck_Jone_Equation\n",
    "    def calc_idf(self, df):\n",
    "        if df > (len(self.docs_length)/2):\n",
    "            return 0\n",
    "        return log((len(self.docs_length)-df+0.5) / (df+0.5), 10)\n",
    "            \n",
    "    '''   End  '''\n",
    "    \n",
    "    def calc_BM_score(self, query_id, query):\n",
    "        \n",
    "        for term in query:\n",
    "            if term in self.vocab:\n",
    "                for doc_id in self.vocab[term]['posting_list']:\n",
    "                    self.score_list[doc_id] += self.queries[query_id][term] * self.vocab[term]['posting_list'][doc_id] * self.vocab[term]['idf']\n",
    "            else:\n",
    "                print(\"%s's %s is not in docs\" % (query_id, term))\n",
    "                            \n",
    "        return [ (key, value) for key, value in sorted(self.score_list.items(),\n",
    "                      key = lambda item:item[1], reverse=True) ]\n",
    "    \n",
    "    def score(self, query_id, query_terms):        \n",
    "        self.score_list = { doc_id : 0 for doc_id in self.score_list }\n",
    "        return self.calc_BM_score(query_id, query_terms)\n",
    "    \n",
    "    \n",
    "    def calc_doc_weight(self): \n",
    "        \n",
    "        # tf(i): term(i) 's frequency\n",
    "        # d(j): document j\n",
    "        # df(i): vocabulary(i) appear in all document's frequcy        \n",
    "        \n",
    "        doc_list = self.get_file_list(self.doc_list_path)\n",
    "        query_list = self.get_file_list(self.query_list_path)\n",
    "        \n",
    "        if self.corpus_type == 'document':  \n",
    "            # construct vocabulary (index term) and calculate document weight  \n",
    "            for doc_id in doc_list:\n",
    "                self.score_list[doc_id] = 0\n",
    "                self.doc_list[doc_id] = self.read_tokenize_data(join(self.doc_fpath, doc_id+'.txt'))\n",
    "                self.docs_length[doc_id] = len(self.doc_list[doc_id])\n",
    "                self.total_doc_length += self.docs_length[doc_id]\n",
    "                doc_words = set()\n",
    "                \n",
    "                for term in self.doc_list[doc_id]:                \n",
    "                    # add vocabulary and calculate df(i)\n",
    "                    if term not in self.vocab:\n",
    "                        inverted_index_info = dict()\n",
    "                        inverted_index_info['posting_list'] = dict()\n",
    "                        inverted_index_info['df'] = 1              \n",
    "                        self.vocab[term] = inverted_index_info\n",
    "                    elif term not in doc_words:\n",
    "                        self.vocab[term]['df'] += 1\n",
    "                    # calculate tf(i) in d(j) \n",
    "                    if term not in doc_words:\n",
    "                        self.vocab[term]['posting_list'][doc_id] = 1\n",
    "                    else:\n",
    "                        self.vocab[term]['posting_list'][doc_id] += 1\n",
    "                    doc_words.add(term)        \n",
    "        elif self.corpus_type == 'query':   \n",
    "            # construct vocabulary (index term)\n",
    "            for query_id in query_list:\n",
    "                self.query_list[query_id] = self.read_tokenize_data(join(self.query_fpath, query_id+'.txt'))\n",
    "                for term in self.query_list[query_id]:\n",
    "                    if term not in self.vocab:\n",
    "                        inverted_index_info = dict()\n",
    "                        inverted_index_info['posting_list'] = dict()\n",
    "                        inverted_index_info['df'] = 0             \n",
    "                        self.vocab[term] = inverted_index_info \n",
    "                        \n",
    "            # calculate document weight\n",
    "            for doc_id in doc_list:\n",
    "                self.score_list[doc_id] = 0\n",
    "                self.docs_length[doc_id] = 0\n",
    "                self.doc_list[doc_id] = self.read_tokenize_data(join(self.doc_fpath, doc_id+'.txt'))\n",
    "                doc_words = set()\n",
    "                for term in self.doc_list[doc_id]:\n",
    "                    if term in self.vocab:\n",
    "                        self.docs_length[doc_id] += 1\n",
    "                self.total_doc_length += self.docs_length[doc_id]\n",
    "\n",
    "                for term in self.doc_list[doc_id]:     \n",
    "                    if term in self.vocab:\n",
    "                        # add vocabulary and calculate df(i)\n",
    "                        if self.vocab[term]['df'] == 0:\n",
    "                            self.vocab[term]['df'] = 1\n",
    "                        elif term not in doc_words:\n",
    "                            self.vocab[term]['df'] += 1\n",
    "                        # calculate tf(i) in d(j) \n",
    "                        if term not in doc_words:\n",
    "                            self.vocab[term]['posting_list'][doc_id] = 1\n",
    "                        else:\n",
    "                            self.vocab[term]['posting_list'][doc_id] += 1\n",
    "                        doc_words.add(term)   \n",
    "\n",
    "        # doc_num: the number N of document\n",
    "        # idf(i): inverse document frequency\n",
    "\n",
    "        self.avg_doclen = self.total_doc_length / len(self.docs_length)  \n",
    "\n",
    "        for doc_id in self.doc_list:\n",
    "            # calculate weighting(current is tf and idf(i)) in d(j)\n",
    "            for index_term in self.vocab:\n",
    "                if doc_id in self.vocab[index_term]['posting_list']:\n",
    "                    doc_tf = self.vocab[index_term]['posting_list'][doc_id]\n",
    "                    # parameters: tf, doc_len\n",
    "                    tf_factor = self.calc_doc_tf(doc_tf, self.docs_length[doc_id])\n",
    "                    idf_factor = self.calc_idf(self.vocab[index_term]['df'])\n",
    "                    self.vocab[index_term]['posting_list'][doc_id] = tf_factor\n",
    "                    if 'idf' not in self.vocab[index_term]:\n",
    "                        self.vocab[index_term]['idf'] = idf_factor\n",
    "                        \n",
    "    def calc_query_weight(self):        \n",
    "        \n",
    "        # tf(i): term i 's frequency\n",
    "        # q: query\n",
    "        # df(i): term i appear in all query's frequency\n",
    "        if self.corpus_type == 'document':\n",
    "            query_list = self.get_file_list(self.query_list_path)\n",
    "            for query_id in query_list:\n",
    "                query_words = set()\n",
    "                query_weight = dict() \n",
    "                query_terms = self.read_tokenize_data(join(self.query_fpath, query_id+'.txt'))\n",
    "                for term in query_terms:\n",
    "                    if term not in query_words:\n",
    "                        query_weight[term] = 1\n",
    "                    else:\n",
    "                        query_weight[term] += 1\n",
    "                    query_words.add(term)\n",
    "                self.queries[query_id] = query_weight\n",
    "\n",
    "                # calculate weighting(current is tf(i)) in q\n",
    "                for term in query_terms:\n",
    "                    if term in self.vocab:\n",
    "                        # parameters: tf\n",
    "                        tf_factor = self.calc_query_tf(self.queries[query_id][term])\n",
    "                        self.queries[query_id][term] = tf_factor\n",
    "                self.rank[query_id] = self.score(query_id, query_terms)\n",
    "                \n",
    "        elif self.corpus_type == 'query':            \n",
    "            for query_id in self.query_list:\n",
    "                query_words = set()\n",
    "                query_weight = dict() \n",
    "                query_terms = self.query_list[query_id]\n",
    "                for term in query_terms:\n",
    "                    if term not in query_words:\n",
    "                        query_weight[term] = 1\n",
    "                    else:\n",
    "                        query_weight[term] += 1\n",
    "                    query_words.add(term)\n",
    "                self.queries[query_id] = query_weight\n",
    "\n",
    "                # calculate weighting(current is tf(i)) in q\n",
    "                for term in query_terms:\n",
    "                    if term in self.vocab:\n",
    "                        # parameters: tf\n",
    "                        tf_factor = self.calc_query_tf(self.queries[query_id][term])\n",
    "                        self.queries[query_id][term] = tf_factor\n",
    "                self.rank[query_id] = self.score(query_id, query_terms)                \n",
    "                        \n",
    "    def output(self, result_path):\n",
    "        with open(result_path, 'w', encoding='UTF-8') as f:\n",
    "            f.write(\"Query,RetrievedDocuments\\n\")\n",
    "            for query_id in self.rank:\n",
    "                # output\n",
    "                f.write(\"%s,\" % query_id)         \n",
    "                for rank_id, rank_score in self.rank[query_id]:           \n",
    "                    # print(rank_id, rank_score)\n",
    "                    f.write(\"%s \" % rank_id)\n",
    "                f.write('\\n')\n",
    "        f.close()        \n",
    "        \n",
    "    def model(self, b, k1, k3, delta=0, corpus_type='document'):\n",
    "        \n",
    "        # tunable parameters\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "        self.k3 = k3\n",
    "        self.delta = delta        \n",
    "        self.corpus_type = corpus_type\n",
    "        \n",
    "        # calculate doc and query weightings\n",
    "        print('Building Model ...')\n",
    "        self.calc_doc_weight()\n",
    "        self.calc_query_weight()\n",
    "        print('Model construction completed !')\n",
    "        # self.view_df()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
