{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, mkdir\n",
    "from os.path import join, isfile, basename\n",
    "from math import log\n",
    "\n",
    "%run tokenization.ipynb\n",
    "\n",
    "class VectorSpaceModel:\n",
    "    def __init__(self):\n",
    "        # variable initial \n",
    "        self.doc_num = 0\n",
    "        self.doc_list = dict()\n",
    "        self.query_list = dict()\n",
    "        self.vocab = dict()\n",
    "        self.queries = dict()\n",
    "        self.score = dict()\n",
    "        \n",
    "        # preprocess tool\n",
    "        self.tokenize = Tokenization()\n",
    "        self.tokenize.load_stopword_userdict('stopwords.txt')\n",
    "        \n",
    "    def get_file_list(self, file_list_path):      \n",
    "        return [ join(file_list_path, file) for file in listdir(file_list_path) if isfile(join(file_list_path, file)) ]\n",
    "    \n",
    "    def read_tokenize_data(self, file_path):\n",
    "        \n",
    "        with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "            text = self.tokenize.cut(f.read().strip('\\n'))\n",
    "        f.close()\n",
    "            \n",
    "        return text.split()\n",
    "    \n",
    "#     def binary_search(self, arr, left, right, x):\n",
    "        \n",
    "#         if right >= left:   \n",
    "#             mid = left + (right - left) // 2\n",
    "#             if arr[mid] == x:\n",
    "#                 return mid            \n",
    "#             elif arr[mid] > x: \n",
    "#                 return binary_search(arr, left, mid-1, x) \n",
    "#             else: \n",
    "#                 return binary_search(arr, mid + 1, right, x)\n",
    "    \n",
    "    def view_df(self):\n",
    "        df = dict()\n",
    "        for term in self.vocab:\n",
    "            df[term] = self.vocab[term]['df']\n",
    "            \n",
    "        df_list = [ (key, value) for key, value in sorted(df.items(),\n",
    "                      key = lambda item:item[1], reverse=True) ]\n",
    "        \n",
    "        for term, df in df_list:\n",
    "            print(term, df)\n",
    "    \n",
    "    def calc_unit_vector_length(self, doc_id):\n",
    "        \n",
    "        square_sum = 0\n",
    "        for term in self.vocab:\n",
    "            if doc_id in self.vocab[term]['posting_list']:\n",
    "                square_sum += (self.vocab[term]['posting_list'][doc_id])**2\n",
    "        unit_vector_length = (square_sum)**(0.5)\n",
    "        print(unit_vector_length)\n",
    "    \n",
    "    def calc_doc_term_num(self, doc_id):\n",
    "        num = 0\n",
    "        for term in self.vocab:\n",
    "            if doc_id in self.vocab[term]['posting_list']:\n",
    "                num += 1\n",
    "        return num\n",
    "    \n",
    "    def calc_doc_tf(self, tf):        \n",
    "        return (tf)\n",
    "    \n",
    "    def calc_doc_idf(self, df):\n",
    "        return (log(self.doc_num / df, 10))\n",
    "    \n",
    "    def calc_doc_weight(self, doc_fpath): \n",
    "        \n",
    "        # tf(i): term(i) 's frequency\n",
    "        # d(j): document j\n",
    "        # df(i): vocabulary(i) appear in all document's frequency        \n",
    "        \n",
    "        self.doc_list = self.get_file_list(doc_fpath)        \n",
    "        \n",
    "        for doc_path in self.doc_list:\n",
    "            doc_id = basename(doc_path).split('.')[0]            \n",
    "            self.score[doc_id] = 0\n",
    "            doc_words = set()\n",
    "            doc_terms = self.read_tokenize_data(doc_path)\n",
    "            for term in doc_terms:\n",
    "                # add vocabulary and calculate df(i)\n",
    "                if term not in self.vocab:\n",
    "                    inverted_index_info = dict()\n",
    "                    inverted_index_info['posting_list'] = dict()\n",
    "                    inverted_index_info['df'] = 1              \n",
    "                    self.vocab[term] = inverted_index_info\n",
    "                elif term not in doc_words:\n",
    "                    self.vocab[term]['df'] += 1\n",
    "                # calculate tf(i) in d(j) \n",
    "                if term not in doc_words:\n",
    "                    self.vocab[term]['posting_list'][doc_id] = 1\n",
    "                else:\n",
    "                    self.vocab[term]['posting_list'][doc_id] += 1\n",
    "                doc_words.add(term)\n",
    "        \n",
    "        \n",
    "        # doc_num: the number N of document\n",
    "        # idf(i): inverse document frequency\n",
    "        # tf-idf(i): term(i) 's tf(i) * idf(i)\n",
    "\n",
    "        self.doc_num = len(self.doc_list)\n",
    "        \n",
    "        for doc_path in self.doc_list:\n",
    "            doc_id = basename(doc_path).split('.')[0]      \n",
    "            # calculate weighting(current is tfidf(i)) in d(j)  and d(j)'s vector length\n",
    "            square_sum = 0 \n",
    "            for index_term in self.vocab:\n",
    "                if doc_id in self.vocab[index_term]['posting_list']:\n",
    "                    tf = self.calc_doc_tf(self.vocab[index_term]['posting_list'][doc_id])\n",
    "                    idf = self.calc_doc_idf(self.vocab[index_term]['df'])\n",
    "                    self.vocab[index_term]['posting_list'][doc_id] = tf * idf\n",
    "                    square_sum += (tf * idf)**2      \n",
    "                    if 'idf' not in self.vocab[index_term]:\n",
    "                        self.vocab[index_term]['idf'] = idf\n",
    "\n",
    "            vector_length = square_sum**(0.5)\n",
    "\n",
    "            # normalize\n",
    "            for index_term in self.vocab:\n",
    "                if doc_id in self.vocab[index_term]['posting_list']:\n",
    "                    self.vocab[index_term]['posting_list'][doc_id] /= vector_length   \n",
    "                    # print(self.vocab[index_term]['posting_list'][doc_id])\n",
    "        \n",
    "        # check unit vector length\n",
    "#         for doc_id in self.doc_list:\n",
    "#             doc_id = basename(doc_path).split('.')[0]      \n",
    "#             self.calc_unit_vector_length(doc_id)\n",
    "                \n",
    "    def calc_cosine_similarity(self, doc_x_path, doc_y_path):\n",
    "        \n",
    "        score = 0\n",
    "        tfidf_path = 'Docs_TF-IDF/'\n",
    "        with open(tfidf_path+doc_x_path, 'r', encoding='UTF-8') as x:\n",
    "            with open(tfidf_path+doc_y_path, 'r', encoding='UTF-8') as y:\n",
    "                for x_line in x.readlines()[2:]:\n",
    "                    for y_line in y.readlines()[2:]:\n",
    "                        x_tfidf = x_line.split()\n",
    "                        y_tfidf = y_line.split()\n",
    "                        if x_tfidf[0] == y_tfidf[0]:\n",
    "                            score += float(x_tfidf[1]) * float(y_tfidf[1])\n",
    "            y.close()\n",
    "        x.close()\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def output_dictionary(self):\n",
    "        \n",
    "        with open('dictionary.txt', 'w', encoding='UTF-8') as f:\n",
    "            f.write(\"%-8s\\t%-20s\\t%-3s\\n\" % (\"t_index\", \"term\", \"df\"))\n",
    "            t_index = 0\n",
    "            self.vocab = dict(sorted(self.vocab.items()))\n",
    "            for index_term in self.vocab:\n",
    "                t_index += 1\n",
    "                f.write(\"%-8d\\t%-20s\\t%-3d\\n\" % (t_index, index_term, self.vocab[index_term]['df']))\n",
    "        f.close()\n",
    "        \n",
    "    def output_doc_weight(self, file_path):\n",
    "        try:\n",
    "            mkdir(file_path)\n",
    "            for doc_path in self.doc_list:\n",
    "                doc_id = basename(doc_path).split('.')[0]\n",
    "                with open(file_path+doc_id+'.txt', 'w', encoding='UTF-8') as f:\n",
    "                    f.write(\"%-8d\\n\" % self.calc_doc_term_num(doc_id))\n",
    "                    f.write(\"%-8s\\t\\t%s\\n\" % (\"t_index\", \"tf-idf\"))\n",
    "                    vocab = list(self.vocab.keys())\n",
    "                    for i in range(1, len(vocab)):\n",
    "                        term = vocab[i-1]\n",
    "                        if doc_id in self.vocab[term]['posting_list']:\n",
    "                            f.write(\"%-8d\\t\\t%f\\n\" % ((i-1), self.vocab[term]['posting_list'][doc_id]))\n",
    "                f.close()\n",
    "        except FileExistsError:\n",
    "            print('directory %s already exists' % file_path)\n",
    "        \n",
    "    def model(self, doc_fpath):\n",
    "        \n",
    "        print('model constucting ...')\n",
    "        self.calc_doc_weight(doc_fpath)\n",
    "        # self.view_df()\n",
    "        self.output_dictionary()\n",
    "        self.output_doc_weight('Docs_TF-IDF/')\n",
    "        print('model constructed!')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
