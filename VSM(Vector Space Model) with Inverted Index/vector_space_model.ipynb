{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from math import log\n",
    "\n",
    "%run tokenization.ipynb\n",
    "\n",
    "class VectorSpaceModel:\n",
    "    def __init__(self):\n",
    "        # variable initial \n",
    "        self.doc_num = 0\n",
    "        self.doc_list = dict()\n",
    "        self.query_list = dict()\n",
    "        self.vocab = dict()\n",
    "        self.queries = dict()\n",
    "        self.score = dict()\n",
    "        self.match_num = dict()\n",
    "        \n",
    "        # preprocess tool\n",
    "        self.tokenize = Tokenization()\n",
    "        \n",
    "    def get_file_list(self, file_list_path):      \n",
    "        \n",
    "        file_list = list()        \n",
    "        with open(file_list_path, 'r', encoding='UTF-8') as f:\n",
    "            for file_id in f.readlines():\n",
    "                file_list.append(file_id.strip('\\n'))\n",
    "        f.close()\n",
    "        \n",
    "        return file_list\n",
    "    \n",
    "    def read_tokenize_data(self, file_path):\n",
    "        \n",
    "        with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "            text = self.tokenize.cut(f.read(), splitnum=False, stopword=False)\n",
    "            # text = f.read()            \n",
    "        f.close()\n",
    "            \n",
    "        return text.split()\n",
    "    \n",
    "    def calc_cosine_similarity(self, query_id, query):\n",
    "        \n",
    "        for term in query:\n",
    "            if term in self.vocab:\n",
    "                for doc_id in self.vocab[term]['posting_list']:\n",
    "                    self.score[doc_id] += self.queries[query_id][term] * self.vocab[term]['posting_list'][doc_id]\n",
    "                    self.match_num[doc_id] += 1\n",
    "            else:\n",
    "                print(\"%s's %s is not in docs\" % (query_id, term))\n",
    "                \n",
    "        for doc_id in self.score:\n",
    "            self.score[doc_id] *= (self.match_num[doc_id] / len(query)) \n",
    "            \n",
    "        return [ (key, value) for key, value in sorted(self.score.items(),\n",
    "                      key = lambda item:item[1], reverse=True) ]\n",
    "    \n",
    "    def view_df(self):\n",
    "        df = dict()\n",
    "        for term in self.vocab:\n",
    "            df[term] = self.vocab[term]['df']\n",
    "            \n",
    "        df_list = [ (key, value) for key, value in sorted(df.items(),\n",
    "                      key = lambda item:item[1], reverse=True) ]\n",
    "        \n",
    "        for term, df in df_list:\n",
    "            print(term, df)\n",
    "    \n",
    "    def calc_unit_vector_length(self, doc_id):\n",
    "        \n",
    "        square_sum = 0\n",
    "        for term in self.vocab:\n",
    "            if doc_id in self.vocab[term]['posting_list']:\n",
    "                square_sum += (self.vocab[term]['posting_list'][doc_id])**2\n",
    "        unit_vector_length = (square_sum)**(0.5)\n",
    "        print(unit_vector_length)\n",
    "        \n",
    "    \n",
    "    def calc_doc_tf(self, tf):        \n",
    "        return (1+log(tf, 2))\n",
    "    \n",
    "    def calc_doc_idf(self, df):\n",
    "        return (log(self.doc_num / df, 10))\n",
    "        \n",
    "    def calc_query_tf(self, tf, tf_list):\n",
    "        return (0.4 + 0.6 * (tf / max(tf_list)))\n",
    "    \n",
    "    def calc_query_idf(self, df):        \n",
    "        return log((self.doc_num / df), 10)\n",
    "    \n",
    "    def calc_doc_weight(self, doc_list_path, doc_fpath): \n",
    "        \n",
    "        # tf(i): term(i) 's frequency\n",
    "        # d(j): document j\n",
    "        # df(i): vocabulary(i) appear in all document's frequency        \n",
    "        \n",
    "        self.doc_list = self.get_file_list(doc_list_path)\n",
    "        \n",
    "        for doc_id in self.doc_list:\n",
    "            self.score[doc_id] = 0\n",
    "            self.match_num[doc_id] = 0\n",
    "            doc_words = set()\n",
    "            doc_terms = self.read_tokenize_data(join(doc_fpath, doc_id+'.txt'))\n",
    "            \n",
    "            for term in doc_terms:                \n",
    "                # add vocabulary and calculate df(i)\n",
    "                if term not in self.vocab:\n",
    "                    inverted_index_info = dict()\n",
    "                    inverted_index_info['posting_list'] = dict()\n",
    "                    inverted_index_info['df'] = 1              \n",
    "                    self.vocab[term] = inverted_index_info\n",
    "                elif term not in doc_words:\n",
    "                    self.vocab[term]['df'] += 1\n",
    "                # calculate tf(i) in d(j) \n",
    "                if term not in doc_words:\n",
    "                    self.vocab[term]['posting_list'][doc_id] = 1\n",
    "                else:\n",
    "                    self.vocab[term]['posting_list'][doc_id] += 1\n",
    "                doc_words.add(term)\n",
    "                \n",
    "        # doc_num: the number N of document\n",
    "        # idf(i): inverse document frequency\n",
    "        # tf-idf(i): term(i) 's tf(i) * idf(i)\n",
    "        \n",
    "        self.doc_num = len(self.doc_list)\n",
    "                \n",
    "        for doc_id in self.doc_list:\n",
    "            # calculate weighting(current is tfidf(i)) in d(j)  and d(j)'s vector length\n",
    "            square_sum = 0 \n",
    "            for index_term in self.vocab:\n",
    "                if doc_id in self.vocab[index_term]['posting_list']:\n",
    "                    tf = self.calc_doc_tf(self.vocab[index_term]['posting_list'][doc_id])\n",
    "                    idf = self.calc_doc_idf(self.vocab[index_term]['df'])\n",
    "                    self.vocab[index_term]['posting_list'][doc_id] = tf * idf\n",
    "                    square_sum += (tf * idf)**2      \n",
    "                    if 'idf' not in self.vocab[index_term]:\n",
    "                        self.vocab[index_term]['idf'] = idf\n",
    "                        \n",
    "            vector_length = square_sum**(0.5)\n",
    "            \n",
    "            # normalize\n",
    "            for index_term in self.vocab:\n",
    "                if doc_id in self.vocab[index_term]['posting_list']:\n",
    "                    self.vocab[index_term]['posting_list'][doc_id] /= vector_length    \n",
    "        \n",
    "        # check unit vector length\n",
    "        # for doc_id in self.doc_list:\n",
    "            # self.calc_unit_vector_length(doc_id)\n",
    "                \n",
    "    def output(self, query_list_path, query_fpath, result_path):\n",
    "        \n",
    "        # tf(i): term i 's frequency\n",
    "        # q: query\n",
    "        # df(i): term i appear in all query's frequency\n",
    "        \n",
    "        self.query_list = self.get_file_list(query_list_path)\n",
    "            \n",
    "        with open(result_path, 'w', encoding='UTF-8') as f:\n",
    "            f.write(\"Query,RetrievedDocuments\\n\")\n",
    "            for query_id in self.query_list:\n",
    "                query_words = set()\n",
    "                query_terms = self.read_tokenize_data(join(query_fpath, query_id+'.txt'))\n",
    "                query_weight = dict() \n",
    "                for term in query_terms:\n",
    "                    if term not in query_words:\n",
    "                        query_weight[term] = 1\n",
    "                    else:\n",
    "                        query_weight[term] += 1\n",
    "                    query_words.add(term)\n",
    "                self.queries[query_id] = query_weight\n",
    "                # calculate weighting(current is tfidf(i)) in q and q's vector length\n",
    "                square_sum = 0\n",
    "                for term in query_terms:\n",
    "                    if term in self.vocab:\n",
    "                        tf = self.calc_query_tf(self.queries[query_id][term], self.queries[query_id].values())\n",
    "                        idf = self.vocab[term]['idf'] # self.calc_query_idf(self.vocab[term]['idf']) \n",
    "                        query_weight[term] = tf * idf\n",
    "                        square_sum += (tf * idf)**2\n",
    "\n",
    "                vector_length = square_sum**(0.5)\n",
    "                # normalize\n",
    "                for term in query_terms:\n",
    "                    self.queries[query_id][term] /= vector_length\n",
    "                    \n",
    "                # output\n",
    "                f.write(\"%s,\" % query_id)\n",
    "                self.score = { doc_id : 0 for doc_id in self.score }\n",
    "                self.match_num = { doc_id : 0 for doc_id in self.match_num }\n",
    "                rank = self.calc_cosine_similarity(query_id, query_terms)\n",
    "                for rank_id, rank_score in rank:\n",
    "                    # print(rank_id, rank_score)\n",
    "                    f.write(\"%s \" % rank_id)\n",
    "                f.write('\\n')\n",
    "        f.close()        \n",
    "        \n",
    "    def model(self, doc_list_path, doc_fpath):\n",
    "        \n",
    "        print('model constucting ...')\n",
    "        self.calc_doc_weight(doc_list_path, doc_fpath)\n",
    "        # self.view_df()\n",
    "        print('model constructed!')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
