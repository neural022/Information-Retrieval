{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "%run tokenization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModel():\n",
    "\n",
    "    def __init__(self):\n",
    "        ''' initial variable '''\n",
    "        # data\n",
    "        self.doc_list = list()\n",
    "        self.query_list = list()\n",
    "        self.queries = list()\n",
    "        self.documents = list()\n",
    "        \n",
    "        # mapping data\n",
    "        self.vocab = dict()\n",
    "        \n",
    "        # document unigram language model use\n",
    "        # self.doc_unigram_prob = None\n",
    "        \n",
    "        # background language model use\n",
    "        self.collection = list()\n",
    "        # self.background_prob = None\n",
    "        \n",
    "        # probablistic latent semantic analysis model use\n",
    "        self.term_doc = None\n",
    "        \n",
    "        # score variable\n",
    "        self.query_doc_prob = dict()\n",
    "        self.rank = dict()\n",
    "        \n",
    "        # preprocess tool \n",
    "        self.tokenize = Tokenization()\n",
    "        \n",
    "    def cut(self, text):\n",
    "        # text = self.tokenize.cut(text, stopword=False, splitnum=False)\n",
    "        return text.split()\n",
    "    \n",
    "    def data_preprocess(self, query_list, doc_list, queries, documents):\n",
    "        \n",
    "        self.query_list = query_list   \n",
    "        self.doc_list = doc_list     \n",
    "        self.query_doc_prob = { doc_id:0 for doc_id in self.doc_list }\n",
    "        \n",
    "        for query in queries:\n",
    "            self.queries.append(self.cut(query))\n",
    "        \n",
    "        for doc in documents:\n",
    "            text_seq = self.cut(doc)\n",
    "            self.documents.append(text_seq)\n",
    "            self.collection += text_seq\n",
    "            \n",
    "        # index term\n",
    "        vocab = list(set(self.collection))\n",
    "        self.vocab = { vocab[i]:i for i in range(0, len(vocab)) }\n",
    "                                                  \n",
    "    # background language model function\n",
    "    def background_model(self):\n",
    "        # cf(i) is collection frequency\n",
    "        # calculate cf(i)        \n",
    "        self.background_prob = np.zeros(len(self.vocab))\n",
    "        for word in self.collection:\n",
    "            self.background_prob[self.vocab[word]] += 1\n",
    "        # calculate background probability \n",
    "        self.background_prob /= len(self.collection)\n",
    "        \n",
    "    def get_background_prob(self, word):\n",
    "        return self.background_prob[self.vocab[word]]\n",
    "    \n",
    "    def sparse_matrix(self, data, row, column):\n",
    "        return sparse.coo_matrix((data, (np.array(row), np.array(column))))\n",
    "        \n",
    "    # document unigram language model function\n",
    "    def doc_model(self):\n",
    "        # term-doc matrix use sparse matrix\n",
    "        # row column coordinate\n",
    "        row = list()\n",
    "        column = list()\n",
    "        term_freq = list()\n",
    "        unigram_prob = list()\n",
    "        \n",
    "        for j in range(0, len(self.documents)):\n",
    "            doc_set = set(self.documents[j])\n",
    "            tf = np.zeros(len(self.vocab))\n",
    "            # calculate tf(i)\n",
    "            for word in self.documents[j]:\n",
    "                tf[self.vocab[word]] += 1\n",
    "            # store sparse\n",
    "            for word_coord in tf.nonzero()[0]:\n",
    "                row.append(word_coord)\n",
    "                column.append(j)\n",
    "                term_freq.append(tf[word_coord])\n",
    "                unigram_prob.append(tf[word_coord] / len(self.documents[j]))\n",
    "        \n",
    "        self.term_doc = self.sparse_matrix(term_freq, row, column)\n",
    "        self.doc_unigram_prob = self.sparse_matrix(unigram_prob, row, column).tocsr()\n",
    "        \n",
    "    def get_unigram_prob(self, word, doc):\n",
    "        row = self.vocab[word]\n",
    "        column = doc\n",
    "        return self.doc_unigram_prob[row, column]\n",
    "    \n",
    "    def sum_topic(self, word, doc):\n",
    "        likelihood = 0\n",
    "        for k in range(self.topic_num):            \n",
    "            likelihood += self.word_topic_prob[self.vocab[word], k] * self.topic_doc_prob[k, doc]\n",
    "        return likelihood\n",
    "              \n",
    "    def prob(self, query, map_at):\n",
    "        \n",
    "        for word in query:\n",
    "            if word in self.vocab:\n",
    "                for j in range(0, len(self.documents)):\n",
    "                    unigram_prob = self.get_unigram_prob(word, j)\n",
    "                    background_prob = self.get_background_prob(word)\n",
    "                    self.query_doc_prob[self.doc_list[j]] += np.log(self.alpha*unigram_prob + self.beta*self.sum_topic(word, j) + (1-self.alpha-self.beta)*background_prob)\n",
    "        \n",
    "        return [ (key, value) for key, value in sorted(self.query_doc_prob.items(),\n",
    "                      key = lambda item:item[1], reverse=True)[:map_at] ]\n",
    "    \n",
    "    def query_likelihood(self, word_topic_prob, topic_doc_prob, topic_num, map_at=1000, alpha=None, beta=None):\n",
    "        \n",
    "        # tuning parameters\n",
    "        self.topic_num = topic_num\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        # PLSA prob\n",
    "        self.word_topic_prob = word_topic_prob\n",
    "        self.topic_doc_prob = topic_doc_prob\n",
    "        \n",
    "        # query likelihood measure\n",
    "        for i in range(0, len(self.queries)):\n",
    "            query = self.queries[i]\n",
    "            self.query_doc_prob = { doc_id:0 for doc_id in self.doc_list }\n",
    "            self.rank[self.query_list[i]] = self.prob(query, map_at)\n",
    "            # print(self.rank[self.quer_list[i]])\n",
    "            \n",
    "    def output(self, result_path):\n",
    "        with open(result_path, 'w', encoding='UTF-8') as f:\n",
    "            f.write(\"Query,RetrievedDocuments\\n\")\n",
    "            for query_id in self.rank:\n",
    "                # output\n",
    "                f.write(\"%s,\" % query_id)\n",
    "                # print(len(self.rank[query_id]))\n",
    "                for rank_id, rank_score in self.rank[query_id]:           \n",
    "                    # print(rank_id, rank_score)\n",
    "                    f.write(\"%s \" % rank_id)\n",
    "                f.write('\\n')\n",
    "        f.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
