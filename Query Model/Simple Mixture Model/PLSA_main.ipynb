{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from numba import jit\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "%run PLSA_model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Calculate Time Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_file_list(file_list_path):\n",
    "\n",
    "    file_list = list()        \n",
    "    with open(file_list_path, 'r', encoding='UTF-8') as f:\n",
    "        for file_id in f.readlines():\n",
    "            file_list.append(file_id.strip('\\n'))\n",
    "    f.close()        \n",
    "    return file_list\n",
    "\n",
    "def load_data(file_list, file_path):\n",
    "    \n",
    "    data = list()\n",
    "    for file_id in file_list:\n",
    "        with open(join(file_path, file_id+'.txt'), 'r', encoding='UTF-8') as f:\n",
    "            data.append(f.read())\n",
    "        f.close()\n",
    "    return data\n",
    "\n",
    "def now_time():\n",
    "    return datetime.datetime.now()\n",
    "\n",
    "def cost_time(start_time, end_time):\n",
    "    cost_time = end_time - start_time\n",
    "    print('Cost time: %s\\n' % cost_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSA Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_initial(row, col):  \n",
    "    # uniform distribution\n",
    "    np_array = np.random.rand(row, col)\n",
    "    '''\n",
    "    e.g prob = [ 0.7, 0.6, 0.3 ]\n",
    "    sum = 1.6\n",
    "    x1 = 0.7 / 1.6 = 0.4375\n",
    "    x2 = 0.6 / 1.6 = 0.375\n",
    "    x3 = 0.3 / 1.6 = 0.1875\n",
    "    Σ summation(x1, x2, x3) = 1 \n",
    "    '''\n",
    "    # axis = 0:sum of row\n",
    "    # axis = 1: sum of column        \n",
    "    return np_array / np_array.sum(axis=0, keepdims=True)\n",
    "        \n",
    "# Probabilistic Latent Semantic Analysis\n",
    "@jit(nopython=True)\n",
    "def plsa(word_topic_prob, topic_doc_prob, coo_row, coo_col, coo_data, topic=1, iter_num=1):\n",
    "    \n",
    "    # term_doc matrix's row is word w(i) \n",
    "    # term_doc matrix's column is document d(j)\n",
    "    \n",
    "    # print(len(coo_row), len(coo_col), len(coo_data))\n",
    "    # P(Tk | wi, dj)\n",
    "    topic_word_doc_prob = np.zeros((len(coo_data), topic_num))\n",
    "    topics_sum = np.zeros(topic)\n",
    "    docs_len = np.zeros(topic_doc_prob.shape[1])\n",
    "    \n",
    "    # EM Algorithm\n",
    "    print(\"Iteration Start:\")\n",
    "    for iter_index in range(iter_num):\n",
    "        # E step\n",
    "        for i in range(len(coo_data)):            \n",
    "            w_coord = coo_row[i]\n",
    "            d_coord = coo_col[i]\n",
    "            topic_prob = np.zeros((topic))  \n",
    "            topic_sum_prob = 0\n",
    "            \n",
    "            for k in range(topic):\n",
    "                # P(wi | Tk) * P(Tk | dj)\n",
    "                topic_prob[k] = word_topic_prob[w_coord, k] * topic_doc_prob[k, d_coord]\n",
    "                topic_sum_prob += topic_prob[k]\n",
    "                \n",
    "            for k in range(topic):\n",
    "                topic_word_doc_prob[i, k] = topic_prob[k] / topic_sum_prob        \n",
    "        \n",
    "        # M step\n",
    "        \n",
    "        # initial zero\n",
    "        topics_sum.fill(0)\n",
    "        word_topic_prob.fill(0)\n",
    "        topic_doc_prob.fill(0)\n",
    "        docs_len.fill(0)\n",
    "        \n",
    "        for i in range(len(coo_data)):\n",
    "            w_coord = coo_row[i]\n",
    "            d_coord = coo_col[i]\n",
    "            term_freq = coo_data[i]\n",
    "            for k in range(topic):\n",
    "                tf_twd = term_freq * topic_word_doc_prob[i, k]\n",
    "                # P(wi | Tk)\n",
    "                word_topic_prob[w_coord, k] += tf_twd\n",
    "                # P(Tk | dj)\n",
    "                topic_doc_prob[k, d_coord] += tf_twd\n",
    "                topics_sum[k] += tf_twd\n",
    "            docs_len[d_coord] += term_freq\n",
    "\n",
    "            \n",
    "        for i in range(word_topic_prob.shape[0]):\n",
    "            for k in range(topic):\n",
    "                word_topic_prob[i, k] /= topics_sum[k]\n",
    "\n",
    "        for k in range(topic):\n",
    "            for j in range(topic_doc_prob.shape[1]):\n",
    "                topic_doc_prob[k, j] /= docs_len[j]\n",
    "                \n",
    "        # log likelihood        \n",
    "        likelihood_sum = 0\n",
    "        for i in range(0, len(coo_data)):            \n",
    "            w_coord = coo_row[i]\n",
    "            d_coord = coo_col[i]\n",
    "            term_freq = coo_data[i]\n",
    "            \n",
    "            likelihood = 0\n",
    "            for k in range(0, topic):                \n",
    "                likelihood += word_topic_prob[w_coord, k] * topic_doc_prob[k, d_coord]\n",
    "            \n",
    "            likelihood_sum += term_freq * np.log(likelihood)\n",
    "        print(\"Iteration #\", iter_index+1, '=', likelihood_sum)\n",
    "    # final\n",
    "    return word_topic_prob, topic_doc_prob\n",
    "\n",
    "# @njit\n",
    "# def plsa_modeling(word_size, doc_size, unigram_prob, word_doc_prob, background_prob, alpha, beta):\n",
    "#     print('#')\n",
    "#     for i in range(word_size):\n",
    "#         for j in range(doc_size):\n",
    "#             key = (np.int64(i), np.int64(j))\n",
    "#             if unigram_prob.get(key) is not None:\n",
    "#                 word_doc_prob[i, j] = alpha*unigram_prob[key] + beta*word_doc_prob[i, j] + (1-alpha-beta)*background_prob[i]\n",
    "#             else:\n",
    "#                 word_doc_prob[i, j] = beta*word_doc_prob[i, j] + (1-alpha-beta)*background_prob[i]\n",
    "\n",
    "#    return word_doc_prob\n",
    "\n",
    "@jit(nopython=True)\n",
    "def plsa_modeling(word_size, doc_size, unigram_prob, word_doc_prob, background_prob, alpha, beta):\n",
    "    print('#')\n",
    "    for i in range(word_size):\n",
    "        for j in range(doc_size):\n",
    "            word_doc_prob[i, j] = alpha*unigram_prob[i, j] + beta*word_doc_prob[i, j] + (1-alpha-beta)*background_prob[i]\n",
    "\n",
    "    return word_doc_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data ...\n",
      "load data finish！\n",
      "\n",
      "Cost time: 0:00:00.432246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "if __name__ == '__main__':\n",
    "    start_time = now_time()    \n",
    "\n",
    "    # Load Data\n",
    "    print('load data ...')    \n",
    "    doc_list = get_file_list('ntust-ir-2020_hw5_new/doc_list.txt')\n",
    "    query_list = get_file_list('ntust-ir-2020_hw5_new/query_list.txt')\n",
    "    docs = load_data(doc_list, 'ntust-ir-2020_hw5_new/docs/')\n",
    "    queries = load_data(query_list, 'ntust-ir-2020_hw5_new/queries/')  \n",
    "    print('load data finish！\\n')\n",
    "    \n",
    "    cost_time(start_time, now_time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    topic = TopicModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data preprocess | background modeling | document modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocess ...\n",
      "data preprocess finish！\n",
      "\n",
      "background modeling ...\n",
      "background modeling finish！\n",
      "\n",
      "document modeling  ...\n",
      "document modeling  finish！\n",
      "\n",
      "Cost time: 0:00:46.081368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    start_time = now_time()    \n",
    "    \n",
    "    # Data Preprocess\n",
    "    print('data preprocess ...')    \n",
    "    topic.data_preprocess(query_list, doc_list, queries, docs)    \n",
    "    print('data preprocess finish！\\n')\n",
    "\n",
    "    # background modeling  \n",
    "    print('background modeling ...')\n",
    "    topic.background_model()    \n",
    "    background_prob = topic.background_prob\n",
    "    print('background modeling finish！\\n')\n",
    "    \n",
    "    # document modeling  \n",
    "    print('document modeling  ...')\n",
    "    topic.doc_model()\n",
    "    doc_unigram_prob = topic.doc_unigram_prob.toarray()\n",
    "    print('document modeling  finish！\\n')\n",
    "    \n",
    "    cost_time(start_time, now_time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLSA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSA training ...\n",
      "154240\n",
      "30000\n",
      "Iteration Start:\n",
      "Iteration # 1 = -100838261.41036318\n",
      "Iteration # 2 = -100721458.46087986\n",
      "Iteration # 3 = -100580098.62736395\n",
      "Iteration # 4 = -100371546.74157852\n",
      "Iteration # 5 = -100040043.07167366\n",
      "Iteration # 6 = -99530295.80967695\n",
      "Iteration # 7 = -98761336.49125214\n",
      "Iteration # 8 = -97645751.25671434\n",
      "Iteration # 9 = -96211888.45785245\n",
      "Iteration # 10 = -94658995.89060335\n",
      "Iteration # 11 = -93229146.72616015\n",
      "Iteration # 12 = -92044877.71902134\n",
      "Iteration # 13 = -91105445.53244871\n",
      "Iteration # 14 = -90360923.27914272\n",
      "Iteration # 15 = -89761566.61258103\n",
      "Iteration # 16 = -89270366.07052375\n",
      "Iteration # 17 = -88861444.2164478\n",
      "Iteration # 18 = -88516806.55574057\n",
      "Iteration # 19 = -88223545.28363496\n",
      "Iteration # 20 = -87971771.11254317\n",
      "Iteration # 21 = -87753687.59785198\n",
      "Iteration # 22 = -87563158.8364695\n",
      "Iteration # 23 = -87395331.07962506\n",
      "Iteration # 24 = -87246302.32337488\n",
      "Iteration # 25 = -87113193.89765729\n",
      "Iteration # 26 = -86994033.6178564\n",
      "Iteration # 27 = -86887094.4695734\n",
      "Iteration # 28 = -86790750.11006674\n",
      "Iteration # 29 = -86703627.81639792\n",
      "Iteration # 30 = -86624406.66952144\n",
      "Iteration # 31 = -86551998.50398116\n",
      "Iteration # 32 = -86485407.10598588\n",
      "Iteration # 33 = -86423901.7026538\n",
      "Iteration # 34 = -86366942.11248073\n",
      "Iteration # 35 = -86313917.05317785\n",
      "Iteration # 36 = -86264344.85062422\n",
      "Iteration # 37 = -86217793.30943769\n",
      "Iteration # 38 = -86174104.86162642\n",
      "Iteration # 39 = -86132939.63018562\n",
      "Iteration # 40 = -86093915.2038992\n",
      "Iteration # 41 = -86056854.04396619\n",
      "Iteration # 42 = -86021849.34750724\n",
      "Iteration # 43 = -85988925.75048865\n",
      "Iteration # 44 = -85957814.8028037\n",
      "Iteration # 45 = -85928321.01892361\n",
      "Iteration # 46 = -85900405.53129113\n",
      "Iteration # 47 = -85874020.65661927\n",
      "Iteration # 48 = -85848955.75424896\n",
      "Iteration # 49 = -85825012.91432245\n",
      "Iteration # 50 = -85802026.08811639\n",
      "Iteration # 51 = -85780054.75304534\n",
      "Iteration # 52 = -85759073.24981144\n",
      "Iteration # 53 = -85738922.23136835\n",
      "Iteration # 54 = -85719613.118557\n",
      "Iteration # 55 = -85701180.7103863\n",
      "Iteration # 56 = -85683374.30120857\n",
      "Iteration # 57 = -85666146.10948394\n",
      "Iteration # 58 = -85649661.96817455\n",
      "Iteration # 59 = -85633896.87900026\n",
      "Iteration # 60 = -85618797.02542278\n",
      "Iteration # 61 = -85604196.47424138\n",
      "Iteration # 62 = -85590058.73892725\n",
      "Iteration # 63 = -85576478.0672636\n",
      "Iteration # 64 = -85563281.85106766\n",
      "Iteration # 65 = -85550463.0040835\n",
      "Iteration # 66 = -85538046.79928182\n",
      "Iteration # 67 = -85525969.54480216\n",
      "Iteration # 68 = -85514323.20915332\n",
      "Iteration # 69 = -85503104.94028926\n",
      "Iteration # 70 = -85492230.04457591\n",
      "Iteration # 71 = -85481669.8299507\n",
      "Iteration # 72 = -85471473.96778888\n",
      "Iteration # 73 = -85461559.31960538\n",
      "Iteration # 74 = -85451976.75931166\n",
      "Iteration # 75 = -85442655.90458374\n",
      "Iteration # 76 = -85433490.04557621\n",
      "Iteration # 77 = -85424543.80608395\n",
      "Iteration # 78 = -85415927.26173846\n",
      "Iteration # 79 = -85407622.58439419\n",
      "Iteration # 80 = -85399571.8569141\n",
      "Iteration # 81 = -85391740.18960932\n",
      "Iteration # 82 = -85384057.37641199\n",
      "Iteration # 83 = -85376491.6260199\n",
      "Iteration # 84 = -85369096.65707907\n",
      "Iteration # 85 = -85361895.29527654\n",
      "Iteration # 86 = -85354868.77301155\n",
      "Iteration # 87 = -85348026.59358066\n",
      "Iteration # 88 = -85341331.04718705\n",
      "Iteration # 89 = -85334800.51580659\n",
      "Iteration # 90 = -85328454.21359675\n",
      "Iteration # 91 = -85322300.72844557\n",
      "Iteration # 92 = -85316349.59828134\n",
      "Iteration # 93 = -85310541.77556881\n",
      "Iteration # 94 = -85304843.2088747\n",
      "Iteration # 95 = -85299236.84149237\n",
      "Iteration # 96 = -85293718.84760727\n",
      "Iteration # 97 = -85288336.61151533\n",
      "Iteration # 98 = -85283151.36588441\n",
      "Iteration # 99 = -85278095.68593189\n",
      "Iteration # 100 = -85273050.88676913\n",
      "Iteration # 101 = -85268011.83477104\n",
      "Iteration # 102 = -85263012.48849519\n",
      "Iteration # 103 = -85258121.11839923\n",
      "Iteration # 104 = -85253363.4509659\n",
      "Iteration # 105 = -85248741.81606905\n",
      "Iteration # 106 = -85244225.20750716\n",
      "Iteration # 107 = -85239796.85924022\n",
      "Iteration # 108 = -85235402.83835784\n",
      "Iteration # 109 = -85231057.4693402\n",
      "Iteration # 110 = -85226830.55973321\n",
      "Iteration # 111 = -85222687.46852806\n",
      "Iteration # 112 = -85218617.95386866\n",
      "Iteration # 113 = -85214555.91345303\n",
      "Iteration # 114 = -85210563.03721629\n",
      "Iteration # 115 = -85206715.68552965\n",
      "Iteration # 116 = -85202988.64662014\n",
      "Iteration # 117 = -85199340.45664361\n",
      "Iteration # 118 = -85195751.63472323\n",
      "Iteration # 119 = -85192202.26201755\n",
      "Iteration # 120 = -85188692.5792984\n",
      "Iteration # 121 = -85185223.04688269\n",
      "Iteration # 122 = -85181751.51632796\n",
      "Iteration # 123 = -85178302.1307553\n",
      "Iteration # 124 = -85174917.42586012\n",
      "Iteration # 125 = -85171586.82035631\n",
      "Iteration # 126 = -85168305.07552364\n",
      "Iteration # 127 = -85165091.09314583\n",
      "Iteration # 128 = -85161927.38848107\n",
      "Iteration # 129 = -85158812.18006445\n",
      "Iteration # 130 = -85155729.37632091\n",
      "Iteration # 131 = -85152618.12989591\n",
      "Iteration # 132 = -85149555.35701978\n",
      "Iteration # 133 = -85146577.08731513\n",
      "Iteration # 134 = -85143611.02469622\n",
      "Iteration # 135 = -85140631.72802764\n",
      "Iteration # 136 = -85137700.83664177\n",
      "Iteration # 137 = -85134878.09275687\n",
      "Iteration # 138 = -85132132.30468358\n",
      "Iteration # 139 = -85129437.47317131\n",
      "Iteration # 140 = -85126821.56631395\n",
      "Iteration # 141 = -85124253.43020792\n",
      "Iteration # 142 = -85121680.3576943\n",
      "Iteration # 143 = -85119093.97130677\n",
      "Iteration # 144 = -85116553.81966767\n",
      "Iteration # 145 = -85114074.10712829\n",
      "Iteration # 146 = -85111609.34749614\n",
      "Iteration # 147 = -85109184.35297118\n",
      "Iteration # 148 = -85106784.66305545\n",
      "Iteration # 149 = -85104382.35676846\n",
      "Iteration # 150 = -85101993.8538874\n",
      "PLSA training finish！\n",
      "\n",
      "Cost time: 0:19:33.497882\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    start_time = now_time()    \n",
    "    \n",
    "    print('PLSA training ...')\n",
    "    # dimensional length\n",
    "    word_size = len(topic.vocab)\n",
    "    doc_size  = len(topic.documents)\n",
    "    topic_num = 256\n",
    "    print(word_size)\n",
    "    print(doc_size)\n",
    "    \n",
    "#     word_topic_prob = np.load('plsa_word_topic_prob.npy')\n",
    "#     topic_doc_prob = np.load('plsa_topic_doc_prob.npy')\n",
    "    # random initial \n",
    "    word_topic_prob = random_initial(word_size, topic_num)\n",
    "    topic_doc_prob  = random_initial(topic_num, doc_size)\n",
    "    # print('for each topic k:', word_topic_prob.sum(axis=0))\n",
    "    # print('for each document j:', topic_doc_prob.sum(axis=0))\n",
    "    \n",
    "    # print(topic.term_doc)\n",
    "    coo_row = topic.term_doc.row\n",
    "    coo_col = topic.term_doc.col\n",
    "    coo_data = topic.term_doc.data\n",
    "    \n",
    "    word_topic_prob, topic_doc_prob = plsa(word_topic_prob, topic_doc_prob, coo_row, coo_col, coo_data, topic_num, iter_num=150)\n",
    "    print('PLSA training finish！\\n')\n",
    "    cost_time(start_time, now_time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summation Topic k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSA summation Topic k ...\n",
      "PLSA summation Topic k finish！\n",
      "\n",
      "Cost time: 0:00:38.433107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    start_time = now_time()    \n",
    "    print('PLSA summation Topic k ...')    \n",
    "    plsa_prob = np.dot(word_topic_prob, topic_doc_prob)\n",
    "    print('PLSA summation Topic k finish！\\n')\n",
    "    cost_time(start_time, now_time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLSA Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#     doc_tuple_unigram_prob = Dict.empty(key_type=types.UniTuple(types.int64, 2), value_type=types.float64,)\n",
    "#     for i in range(len(doc_unigram_prob.data)):\n",
    "#         w_coord = doc_unigram_prob.row[i]\n",
    "#         d_coord = doc_unigram_prob.col[i]\n",
    "#         doc_tuple_unigram_prob[(w_coord, d_coord)] = doc_unigram_prob.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSA likelihood ...\n",
      "#\n",
      "PLSA likelihood finish！\n",
      "\n",
      "Cost time: 0:00:07.432730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    start_time = now_time()    \n",
    "    print('PLSA likelihood ...')    \n",
    "    plsa_prob = plsa_modeling(word_size, doc_size, doc_unigram_prob, plsa_prob, background_prob, 0.4, 0.5)\n",
    "    print('PLSA likelihood finish！\\n')\n",
    "    cost_time(start_time, now_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154240, 30000)\n"
     ]
    }
   ],
   "source": [
    "    print(plsa_prob.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query likelihood measure P(q|dj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query likelihood measure ...\n",
      "query likelihood measure finish！\n",
      "\n",
      "Cost time: 0:00:17.325944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    start_time = now_time()    \n",
    "\n",
    "    print('query likelihood measure ...')\n",
    "    # parameter: map@num\n",
    "    topic.query_likelihood(plsa_prob, 5000)\n",
    "    print('query likelihood measure finish！\\n')\n",
    "\n",
    "    cost_time(start_time, now_time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    np.save('plsa_document_unigram_model', doc_unigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    np.save('plsa_background_model', background_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    pd.DataFrame({'vocab':list(topic.vocab.keys())}).to_csv('plsa_vocab', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    np.save('plsa_word_topic_prob', word_topic_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    np.save('plsa_topic_doc_prob', topic_doc_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    np.save('plsa_prob', plsa_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    topic.output('PLSA_model.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
